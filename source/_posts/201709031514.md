---
layout: 萌新学习Python爬取B站弹幕+R语言分词demo说明
title: 萌新学习Python爬取B站弹幕+R语言分词demo说明
date: 2017.09.03 15:14
tags:
    - Python
---
## 写在前面
之前在简书首页看到了Python爬虫的介绍，于是就想着爬取B站弹幕并绘制词云，因此有了这样一个简单的尝试，从搭建环境到跑通demo，不懂语法，不知含义，装好环境，查到API，跑通Demo，就是目标！纯零基础萌新！
[demo地址](https://github.com/a67c/pyBilibilBarrage)(只有python的demo，R的没有上传)

[关于环境的安装及调试过程中遇到的问题记录请移步](http://www.jianshu.com/p/2f2b92783c5b)

## Python爬取B站弹幕
### 环境说明

**windows8.1 x64+python3.6+scrapy1.4**

参考文档：

[scrapy github](https://github.com/scrapy/scrapy/blob/1.4/docs/topics/selectors.rst)

[scrapy document](https://doc.scrapy.org/en/latest/topics/selectors.html?highlight=extract)

[scrapy爬虫框架入门实例](http://blog.csdn.net/zjiang1994/article/details/52779537)

### 步骤说明

* 安装python3.6
* 安装scrapy1.4
* 建立scrapy demo
* 跑通demo遇到问题、解决问题
* 更改demo为B站弹幕爬取demo
我这边是按照参考文档中 [scrapy爬虫框架入门实例](http://blog.csdn.net/zjiang1994/article/details/52779537)这个demo来做的，这个文章里面无论是介绍还是`scrapy`的入门都非常详细，建议大家按照这个来入门，但是由于慕课网的结构样式以及更改了，所以demo是跑不起来的，因此我换成了爬取B站的弹幕demo。截止2017年9月2日亲测可跑通。

### Demo说明
**1.  安装scrapy成功之后建立项目`scrapytest`**
```
scrapy startproject scrapytest
```
**2. demo目录**
本demo目录仅保留当前demo可用的文件，且文件名字不同于`scrapy`自动生成的文件名字，对于未涉及到的文件进行了删除
```
│  scrapy.cfg//项目的配置文件
└─scrapytest
    │  CourseItems.py//定义一个容器保存要爬取的数据
    │  MyPipelines.py//项目中的pipelines文件.
    │  settings.py//项目中的设置文件.
    ├─spiders
    │  │  data.json//爬取数据生成的文件
    │  └─ Myspider.py//爬虫主代码
```
**3. demo代码**

 **创建CourseItems.py文件**
定义一个容器保存要爬取的数据。为了定义常用的输出数据，`Scrapy`提供了`Item`类。`Item`对象是种简单的容器，保存了爬取到得数据。 其提供了 类似于词典(dictionary-like)的API以及用于声明可用字段的简单语法。由于最后输出的只要弹幕的内容，所以容器中只定义了弹幕的内容

```
#引入文件
import scrapy
class CourseItem(scrapy.Item):
    #弹幕内容
    content = scrapy.Field()
```

 **编写爬取代码Myspider.py**
* bilibili的弹幕是在xml文件里，每个视频都有其对应的cid和aid，我们取到cid中的数字放入`http://comment.bilibili.com/+cid+.xml`,即可得到该视频对应的cid。
cid取法：cid在源码中是没有找到的，目前我的做法是在页面上F12，然后查找cid，该cid即为弹幕页的标识，如果有可以通过代码查到的方法，还请告知。目前例子中的cid有1000多条弹幕，建议大家换个少的进行测试。

![cid查找方法](http://upload-images.jianshu.io/upload_images/1094385-e060510f5149b12e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
* 弹幕的xml文件结构非常简单，所以通过Xpath简单解析即可

![弹幕的xml文件结构](http://upload-images.jianshu.io/upload_images/1094385-27d44a81c00fc3cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```
import scrapy
#引入容器
from scrapytest.CourseItems import CourseItem

class Myspider(scrapy.Spider):
    #设置name
    name = "Myspider" //启动项目时所用name
    #设定域名
    allowed_domains = ["bilibili.com"]
    #填写爬取地址
    start_urls = ["http://comment.bilibili.com/2015358.xml"]
    #编写爬取方法
    def parse(self, response):
        #实例一个容器保存爬取的信息
        item = CourseItem()
        #这部分是爬取部分，使用xpath的方式选择信息，具体方法根据网页结构而定
        #直接爬取弹幕内容
        str0 = ''
        for box in response.xpath('/i/d/text()'):
            #获取每一条弹幕内容
            str0 += box.extract()+',';
            #返回信息
        item['content'] = str0;//最后输出的结构是值：字符串的结构，详细见输出图
        yield item
```
**编写MyPipelines.py处理数据**
当成功获取信息后，要进行信息的验证、储存等工作，这里只进行简单的将数据存储在json中的操作。
```
#引入文件
from scrapy.exceptions import DropItem
import json

class MyPipeline(object):
    def __init__(self):
        #打开文件
        self.file = open('data.json', 'w', encoding='utf-8')
    #该方法用于处理数据
    def process_item(self, item, spider):
        #读取item中的数据
        line = json.dumps(dict(item), ensure_ascii=False) + "\n"
        #写入文件
        self.file.write(line)
        #返回item
        return item
    #该方法在spider被开启时被调用。
    def open_spider(self, spider):
        pass
    #该方法在spider被关闭时被调用。
    def close_spider(self, spider):
        pass
```
**注册Pipeline** 
找到`settings.py`文件,这个文件时爬虫的配置文件，在其中添加
 ```
ITEM_PIPELINES = {
    'scrapytest.MyPipelines.MyPipeline': 300,
}
```
上面的代码用于注册`Pipeline`，其中`scrapytest.MyPipelines.MyPipeline`为你要注册的类，右侧的’300’为该`Pipeline`的优先级，范围1～1000，越小越先执行。(*ps：这个并没有详细了解*)
**4. 运行demo**
在`Myspider.py`的同级下执行cmd控制台，运行一下命令。
```
scrapy crawl MySpider
```
**5. 运行结果**
这是一个json的文件，json文件的输出结构更改在`Myspider.py`中，我改成这种通过逗号来连接每一条弹幕时是为了之后方便分词。大家也可以把代码改了改成另一种展示方式

![便于分词的展示方式](http://upload-images.jianshu.io/upload_images/1094385-3729642445b03ed2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![另一种方式](http://upload-images.jianshu.io/upload_images/1094385-074555839887ab2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### Python爬取end
到此python爬取B站弹幕demo结束，接下来我们通过拿到的json文件去R语言中进行分词。

##R语言分词实例
### 环境说明

**window8.1 x64+R3.4.2+jiebaR插件+rJson插件+RStudio编辑器+wordcloud2插件**

[R语言官网](https://www.r-project.org/)

[Rstudio下载地址](https://www.rstudio.com/products/rstudio/download/)

[R语言中文分司包jiabaR](http://blog.fens.me/r-word-jiebar/)

[R和json的傻瓜式编程](http://blog.fens.me/r-json-rjson/)

[R语言w3c教程](https://www.w3cschool.cn/r/r_overview.html)

[jiebaR中午分词文档](http://qinwenfeng.com/jiebaR/section-1-1.html)

[wordcloud2 gtihub](https://github.com/Lchiffon/wordcloud2)

[R语言︱文本挖掘——词云wordcloud2包](http://www.cnblogs.com/nxld/p/6344233.html?utm_source=itdadao&utm_medium=referral)

### 步骤说明
* 安装R、Rstudio、jiebaR、rJSON
* 引入JSON文件
* 分词处理
* 停止词处理
* 过滤数字及字母
* 产生数据
* 调用wordcloud2绘制词云
关于`jiebaR`分词基本是按照[R语言中文分司包jiabaR](http://blog.fens.me/r-word-jiebar/)这个博客的demo来进行的。该博文中对于`jiebaR`的各种函数介绍的非常全面，因此下面demo将不对代码内容进行详细介绍。demo中的各种路径请自行更改。

### demo说明
只有一个jiebaR.R文件即完成了分词和绘制词云，代码如下：
```
#调入分词的库
library("jiebaR")
library("rjson")

#这里读取的`python`爬取的`json`文件，拿道了对象中`content`键的值，该值是一长串字符串，在爬虫输出的时候通过逗号来连接字符串，因此分词时是通过逗号进行的分词
myfile<-fromJSON(file = "F:/gitlab/py/scrapytest/scrapytest/spiders/data.json")$content

#预处理，这步可以将读入的文本转换为可以分词的字符，本demo通过逗号进行分词
 myfile.res<-myfile[myfile!=","]

#调用分词引擎worker函数  stop_word为停止词设置
wk = worker(stop_word ='F:/R/stopw.txt')

#segment为分词结果
segment = wk[myfile.res]

#对于分词结果进行正则过滤，去掉数字及字母
segment = gsub("[a-zA-Z\\/\\.0-9]+","",segment)

#计算词频，该data即为传入词云的数据
data <- freq(segment)

#引入wordcloud2，在引入之前请先安装
 library(wordcloud2)  

#调用wordcloud2函数绘制词云，该函数参数在github已有介绍
  wordcloud2(data,size = 1, fontFamily = "微软雅黑",color = "random-light"")

```

![计算词频后的结果](http://upload-images.jianshu.io/upload_images/1094385-9024949a3137c79c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


![我一开始万万没有想到我分出来会这么丑](http://upload-images.jianshu.io/upload_images/1094385-f9d22e28a612a9bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 问题说明

**1.计算词频**

由于弹幕的条数比较多，分词过滤后的词频很多，没有细查找如何再进一步的排序过滤筛选词，所以导致词云的结果并不是很好

**2. 关键词提取**

个人认为通过关键字提取出的词云会更好一旦，`jiabaR`提供了关键字提取的方法及提取的结果，结果上面是词语出现频率。

```
#提取150个关键字
keys = worker("keywords",topn=150)
#关键字结果
re = vector_keywords(segment,keys)
```
![提取出的关键字结果](http://upload-images.jianshu.io/upload_images/1094385-d6d5dae7d30132e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
从图上可以看出这个关键词比较适合用来做词云，但是这里遇到了问题，关键字的结果时`vector`类型，并不能直接作为`wordcloud`的参数，从测试结果上来看`wordcloud`的参数接收`data.frame`类型，且要有词的内容和词频，当我通过如下代码将`vector`类型转换为`data.frame`时，并将结果输出到了`csv`的文件后发现，输出的内容并没有词频。**没有词频就无法通过`wordcloud`来进行绘制！！！**,**求指教如何将关键词放入`wordcloud`进行绘制！！！**

```
#re为调用vector_keywords产生的结果
data.frame(re);
#将结果输出到文件中
write.csv(data.frame(re),"F:/R/2345.csv",row.names = T)
```

![通过调用关键词函数vector_keywords产生的结果](http://upload-images.jianshu.io/upload_images/1094385-87adec761690384e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**3. 提出出的词语如何能文字更多**
在做词库处理时，我这边用了搜狗的词库替换了`jiabaR`的原来词库，因此可以出现类似于`神罗天征`这样的四字词语，在原来的词库里，连`宇智波`都是被分开的！但是如何把很短的一句话也提取出来呢，从最开始的弹幕可以看到，原文件中是有大量的重复的一句话，除了自己在搜狗词包之外设置固定的词语短句，不知道还有没有别的方法，欢迎指导。

## R语言分词end
最后的那个图被我做的太丑了，简直影响观看，我如果一开始能预料到分出来会这么丑……我万万不会去分的，而且现在做云文字的网站都自带分词好像是，所以……所以我也不知道我这是在干嘛……。**如有错误还请指教！**

在整个过程中遇到的调试记录请移步[Python爬取B站弹幕+R语言分词安装调试报错记录](/2017/09/03/201709031632/)




